{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7912594,"sourceType":"datasetVersion","datasetId":4648697}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text 2 Gloss\n\nhttps://www.kaggle.com/datasets/bipinkrishna/aslg-pc12\n\nhttps://github.com/imatge-upc/speech2signs-2017-nmt\n\nhttps://github.com/arunnair411/Speech-to-AS","metadata":{}},{"cell_type":"code","source":"!cp -r /kaggle/input/aslg-pc12/ ASLG-PC12/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install spacy==2.3.9\n\nreq = \"\"\"\nbackcall\nblis\ncatalogue\ncertifi\nchardet\ncymem\ndecorator\nfuture\nidna\nimportlib-metadata\nipdb\nipython\nipython-genutils\njedi\nmurmurhash\nnumpy\nparso\npexpect\npickleshare\nplac\npreshed\nprompt-toolkit\nptyprocess\nPygments\nrequests\nsentencepiece==0.2.0\nsix\nsrsly\nthinc\ntorch\ntorchtext==0.6.0\ntqdm\ntraitlets\nurllib3\nwasabi\nwcwidth\nzipp\n\"\"\"\n\nwith open(\"req.txt\",\"w\") as f:\n    f.write(req)\n\n!pip install -r req.txt\n!python3 -m spacy download en","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport argparse\nimport logging\nimport sys\nimport urllib\nimport math\nimport time\n\nimport pickle\nfrom tqdm import tqdm\nimport numpy as np\n\nimport codecs\nimport spacy\nimport torch\nimport tarfile\n\nfrom torchtext.data import Field, Dataset, BucketIterator, Example\nfrom torchtext.datasets import TranslationDataset\nfrom torchtext.data.metrics import bleu_score\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformers","metadata":{}},{"cell_type":"code","source":"# Constants\n\nclass Constants:\n    PAD_WORD = '<blank>'\n    UNK_WORD = '<unk>'\n    BOS_WORD = '<s>'\n    EOS_WORD = '</s>'\n\n# Layers\n\nclass EncoderLayer(nn.Module):\n    ''' Compose with two layers '''\n\n    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n        super(EncoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n    def forward(self, enc_input, slf_attn_mask=None):\n        enc_output, enc_slf_attn = self.slf_attn(\n            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n        enc_output = self.pos_ffn(enc_output)\n        return enc_output, enc_slf_attn\n\n\nclass DecoderLayer(nn.Module):\n    ''' Compose with three layers '''\n\n    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n        super(DecoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n    def forward(\n            self, dec_input, enc_output,\n            slf_attn_mask=None, dec_enc_attn_mask=None):\n        dec_output, dec_slf_attn = self.slf_attn(\n            dec_input, dec_input, dec_input, mask=slf_attn_mask)\n        dec_output, dec_enc_attn = self.enc_attn(\n            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n        dec_output = self.pos_ffn(dec_output)\n        return dec_output, dec_slf_attn, dec_enc_attn\n\n\n# Models\n\ndef get_pad_mask(seq, pad_idx):\n    return (seq != pad_idx).unsqueeze(-2)\n\n\ndef get_subsequent_mask(seq):\n    ''' For masking out the subsequent info. '''\n    sz_b, len_s = seq.size()\n    subsequent_mask = (1 - torch.triu(\n        torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n    return subsequent_mask\n\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_hid, n_position=200):\n        super(PositionalEncoding, self).__init__()\n\n        # Not a parameter\n        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_hid))\n\n    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n        ''' Sinusoid position encoding table '''\n        # TODO: make it with torch instead of numpy\n\n        def get_position_angle_vec(position):\n            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n\n        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n        return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n\n    def forward(self, x):\n        return x + self.pos_table[:, :x.size(1)].clone().detach()\n\n\nclass Encoder(nn.Module):\n    ''' A encoder model with self attention mechanism. '''\n\n    def __init__(\n            self, n_src_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n            d_model, d_inner, pad_idx, dropout=0.1, n_position=200):\n\n        super().__init__()\n\n        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx)\n        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n        self.dropout = nn.Dropout(p=dropout)\n        self.layer_stack = nn.ModuleList([\n            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n            for _ in range(n_layers)])\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n    def forward(self, src_seq, src_mask, return_attns=False):\n\n        enc_slf_attn_list = []\n\n        # -- Forward\n        \n        enc_output = self.dropout(self.position_enc(self.src_word_emb(src_seq)))\n        enc_output = self.layer_norm(enc_output)\n\n        for enc_layer in self.layer_stack:\n            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask)\n            enc_slf_attn_list += [enc_slf_attn] if return_attns else []\n\n        if return_attns:\n            return enc_output, enc_slf_attn_list\n        return enc_output,\n\n\nclass Decoder(nn.Module):\n    ''' A decoder model with self attention mechanism. '''\n\n    def __init__(\n            self, n_trg_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n            d_model, d_inner, pad_idx, n_position=200, dropout=0.1):\n\n        super().__init__()\n\n        self.trg_word_emb = nn.Embedding(n_trg_vocab, d_word_vec, padding_idx=pad_idx)\n        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n        self.dropout = nn.Dropout(p=dropout)\n        self.layer_stack = nn.ModuleList([\n            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n            for _ in range(n_layers)])\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n    def forward(self, trg_seq, trg_mask, enc_output, src_mask, return_attns=False):\n\n        dec_slf_attn_list, dec_enc_attn_list = [], []\n\n        # -- Forward\n        dec_output = self.dropout(self.position_enc(self.trg_word_emb(trg_seq)))\n        dec_output = self.layer_norm(dec_output)\n\n        for dec_layer in self.layer_stack:\n            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n                dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)\n            dec_slf_attn_list += [dec_slf_attn] if return_attns else []\n            dec_enc_attn_list += [dec_enc_attn] if return_attns else []\n\n        if return_attns:\n            return dec_output, dec_slf_attn_list, dec_enc_attn_list\n        return dec_output,\n\n\nclass Transformer(nn.Module):\n    ''' A sequence to sequence model with attention mechanism. '''\n\n    def __init__(\n            self, n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,\n            d_word_vec=512, d_model=512, d_inner=2048,\n            n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1, n_position=200,\n            trg_emb_prj_weight_sharing=True, emb_src_trg_weight_sharing=True):\n\n        super().__init__()\n\n        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx\n\n        self.encoder = Encoder(\n            n_src_vocab=n_src_vocab, n_position=n_position,\n            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n            pad_idx=src_pad_idx, dropout=dropout)\n\n        self.decoder = Decoder(\n            n_trg_vocab=n_trg_vocab, n_position=n_position,\n            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n            pad_idx=trg_pad_idx, dropout=dropout)\n\n        self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=False)\n\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p) \n\n        assert d_model == d_word_vec, \\\n        'To facilitate the residual connections, \\\n         the dimensions of all module outputs shall be the same.'\n\n        self.x_logit_scale = 1.\n        if trg_emb_prj_weight_sharing:\n            # Share the weight between target word embedding & last dense layer\n            self.trg_word_prj.weight = self.decoder.trg_word_emb.weight\n            self.x_logit_scale = (d_model ** -0.5)\n\n        if emb_src_trg_weight_sharing:\n            self.encoder.src_word_emb.weight = self.decoder.trg_word_emb.weight\n\n\n    def forward(self, src_seq, trg_seq):\n\n        src_mask = get_pad_mask(src_seq, self.src_pad_idx)\n        trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx) & get_subsequent_mask(trg_seq)\n\n        enc_output, *_ = self.encoder(src_seq, src_mask)\n        dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)\n        seq_logit = self.trg_word_prj(dec_output) * self.x_logit_scale\n\n        return seq_logit.view(-1, seq_logit.size(2))\n\n\n# Modules\n\nclass ScaledDotProductAttention(nn.Module):\n    ''' Scaled Dot-Product Attention '''\n\n    def __init__(self, temperature, attn_dropout=0.1):\n        super().__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n\n    def forward(self, q, k, v, mask=None):\n\n        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n\n        if mask is not None:\n            attn = attn.masked_fill(mask == 0, -1e9)\n\n        attn = self.dropout(F.softmax(attn, dim=-1))\n        output = torch.matmul(attn, v)\n\n        return output, attn\n\n    \n# Optim\n\nclass ScheduledOptim():\n    '''A simple wrapper class for learning rate scheduling'''\n\n    def __init__(self, optimizer, init_lr, d_model, n_warmup_steps):\n        self._optimizer = optimizer\n        self.init_lr = init_lr\n        self.d_model = d_model\n        self.n_warmup_steps = n_warmup_steps\n        self.n_steps = 0\n\n\n    def step_and_update_lr(self):\n        \"Step with the inner optimizer\"\n        self._update_learning_rate()\n        self._optimizer.step()\n\n\n    def zero_grad(self):\n        \"Zero out the gradients with the inner optimizer\"\n        self._optimizer.zero_grad()\n\n\n    def _get_lr_scale(self):\n        d_model = self.d_model\n        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n\n\n    def _update_learning_rate(self):\n        ''' Learning rate scheduling per step '''\n\n        self.n_steps += 1\n        lr = self.init_lr * self._get_lr_scale()\n\n        for param_group in self._optimizer.param_groups:\n            param_group['lr'] = lr\n\n\n# SubLayers\n\nclass MultiHeadAttention(nn.Module):\n    ''' Multi-Head Attention module '''\n\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super().__init__()\n\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n\n        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n\n        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n\n    def forward(self, q, k, v, mask=None):\n\n        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n\n        residual = q\n\n        # Pass through the pre-attention projection: b x lq x (n*dv)\n        # Separate different heads: b x lq x n x dv\n        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n\n        # Transpose for attention dot product: b x n x lq x dv\n        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n\n        if mask is not None:\n            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n\n        q, attn = self.attention(q, k, v, mask=mask)\n\n        # Transpose to move the head dimension back: b x lq x n x dv\n        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n        q = self.dropout(self.fc(q))\n        q += residual\n\n        q = self.layer_norm(q)\n\n        return q, attn\n\n\nclass PositionwiseFeedForward(nn.Module):\n    ''' A two-feed-forward-layer module '''\n\n    def __init__(self, d_in, d_hid, dropout=0.1):\n        super().__init__()\n        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n\n        residual = x\n\n        x = self.w_2(F.relu(self.w_1(x)))\n        x = self.dropout(x)\n        x += residual\n\n        x = self.layer_norm(x)\n\n        return x\n\n\n# Translator\n\nclass Translator(nn.Module):\n    ''' Load a trained model and translate in beam search fashion. '''\n\n    def __init__(\n            self, model, beam_size, max_seq_len,\n            src_pad_idx, trg_pad_idx, trg_bos_idx, trg_eos_idx):\n        \n\n        super(Translator, self).__init__()\n\n        self.alpha = 0.7\n        self.beam_size = beam_size\n        self.max_seq_len = max_seq_len\n        self.src_pad_idx = src_pad_idx\n        self.trg_bos_idx = trg_bos_idx\n        self.trg_eos_idx = trg_eos_idx\n\n        self.model = model\n        self.model.eval()\n\n        self.register_buffer('init_seq', torch.LongTensor([[trg_bos_idx]]))\n        self.register_buffer(\n            'blank_seqs', \n            torch.full((beam_size, max_seq_len), trg_pad_idx, dtype=torch.long))\n        self.blank_seqs[:, 0] = self.trg_bos_idx\n        self.register_buffer(\n            'len_map', \n            torch.arange(1, max_seq_len + 1, dtype=torch.long).unsqueeze(0))\n\n\n    def _model_decode(self, trg_seq, enc_output, src_mask):\n        trg_mask = get_subsequent_mask(trg_seq)\n        dec_output, *_ = self.model.decoder(trg_seq, trg_mask, enc_output, src_mask)\n        return F.softmax(self.model.trg_word_prj(dec_output), dim=-1)\n\n\n    def _get_init_state(self, src_seq, src_mask):\n        beam_size = self.beam_size\n\n        enc_output, *_ = self.model.encoder(src_seq, src_mask)\n        dec_output = self._model_decode(self.init_seq, enc_output, src_mask)\n        \n        best_k_probs, best_k_idx = dec_output[:, -1, :].topk(beam_size)\n\n        scores = torch.log(best_k_probs).view(beam_size)\n        gen_seq = self.blank_seqs.clone().detach()\n        gen_seq[:, 1] = best_k_idx[0]\n        enc_output = enc_output.repeat(beam_size, 1, 1)\n        return enc_output, gen_seq, scores\n\n\n    def _get_the_best_score_and_idx(self, gen_seq, dec_output, scores, step):\n        assert len(scores.size()) == 1\n        \n        beam_size = self.beam_size\n\n        # Get k candidates for each beam, k^2 candidates in total.\n        best_k2_probs, best_k2_idx = dec_output[:, -1, :].topk(beam_size)\n\n        # Include the previous scores.\n        scores = torch.log(best_k2_probs).view(beam_size, -1) + scores.view(beam_size, 1)\n\n        # Get the best k candidates from k^2 candidates.\n        scores, best_k_idx_in_k2 = scores.view(-1).topk(beam_size)\n \n        # Get the corresponding positions of the best k candidiates.\n        best_k_r_idxs, best_k_c_idxs = best_k_idx_in_k2 // beam_size, best_k_idx_in_k2 % beam_size\n        best_k_idx = best_k2_idx[best_k_r_idxs, best_k_c_idxs]\n\n        # Copy the corresponding previous tokens.\n        gen_seq[:, :step] = gen_seq[best_k_r_idxs, :step]\n        # Set the best tokens in this beam search step\n        gen_seq[:, step] = best_k_idx\n\n        return gen_seq, scores\n\n\n    def translate_sentence(self, src_seq):\n        # Only accept batch size equals to 1 in this function.\n        # TODO: expand to batch operation.\n        assert src_seq.size(0) == 1\n\n        src_pad_idx, trg_eos_idx = self.src_pad_idx, self.trg_eos_idx \n        max_seq_len, beam_size, alpha = self.max_seq_len, self.beam_size, self.alpha \n\n        with torch.no_grad():\n            src_mask = get_pad_mask(src_seq, src_pad_idx)\n            enc_output, gen_seq, scores = self._get_init_state(src_seq, src_mask)\n\n            ans_idx = 0   # default\n            for step in range(2, max_seq_len):    # decode up to max length\n                dec_output = self._model_decode(gen_seq[:, :step], enc_output, src_mask)\n                gen_seq, scores = self._get_the_best_score_and_idx(gen_seq, dec_output, scores, step)\n\n                # Check if all path finished\n                # -- locate the eos in the generated sequences\n                eos_locs = gen_seq == trg_eos_idx   \n                # -- replace the eos with its position for the length penalty use\n                seq_lens, _ = self.len_map.masked_fill(~eos_locs, max_seq_len).min(1)\n                # -- check if all beams contain eos\n                if (eos_locs.sum(1) > 0).sum(0).item() == beam_size:\n                    # TODO: Try different terminate conditions.\n                    _, ans_idx = scores.div(seq_lens.float() ** alpha).max(0)\n                    ans_idx = ans_idx.item()\n                    break\n        return gen_seq[ans_idx][:seq_lens[ans_idx]].tolist()\n","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Process","metadata":{}},{"cell_type":"code","source":"class PreSettings:\n    lang_src = \"en\"\n    lang_trg = \"en\"\n    save_data = \"text2gloss_data.pkl\"\n\n    data_src = None\n    data_trg = None\n\n    max_len = 100\n    min_word_count = 3\n\n    keep_case = False\n    share_vocab = True\n\npreOpt = PreSettings()\nsrc_lang_model = spacy.load(preOpt.lang_src)\ntrg_lang_model = spacy.load(preOpt.lang_trg)\n\nSTOP_WORDS = ['X-', 'DESC-']\nMAX_LEN = preOpt.max_len\nMIN_FREQ = preOpt.min_word_count\n\ndef tokenize_src(text):\n    for w in STOP_WORDS:\n        text = text.replace(w, '')\n    return [tok.text for tok in src_lang_model.tokenizer(text)]\n\ndef tokenize_trg(text):\n    for w in STOP_WORDS:\n        text = text.replace(w, '')\n    return [tok.text for tok in trg_lang_model.tokenizer(text)]\n\ndef filter_examples_with_length(x):\n    return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n\nSRC = Field(\n    tokenize=tokenize_src, lower=not preOpt.keep_case,\n    pad_token=Constants.PAD_WORD, init_token=Constants.BOS_WORD, eos_token=Constants.EOS_WORD)\n\nTRG = Field(\n    tokenize=tokenize_trg, lower=not preOpt.keep_case,\n    pad_token=Constants.PAD_WORD, init_token=Constants.BOS_WORD, eos_token=Constants.EOS_WORD)\n\nTRAIN_SRC_FN = 'ASLG-PC12/ENG-ASL_Train.en'\nTRAIN_TRG_FN = 'ASLG-PC12/ENG-ASL_Train.asl'\nVAL_SRC_FN = 'ASLG-PC12/ENG-ASL_Dev.en' \nVAL_TRG_FN = 'ASLG-PC12/ENG-ASL_Dev.asl'\nTEST_SRC_FN = 'ASLG-PC12/ENG-ASL_Test.en'\nTEST_TRG_FN = 'ASLG-PC12/ENG-ASL_Test.asl'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess():\n    \n    assert not any([preOpt.data_src, preOpt.data_trg]), 'Custom data input is not support now.'\n    assert not any([preOpt.data_src, preOpt.data_trg]) or all([preOpt.data_src, preOpt.data_trg])\n\n    if not all([preOpt.data_src, preOpt.data_trg]):\n        assert {preOpt.lang_src, preOpt.lang_trg} == {'en', 'en'}\n    else:\n        # Pack custom txt file into example datasets\n        raise NotImplementedError\n\n    with open(TRAIN_SRC_FN, 'r') as f:\n        train_src = list(f)\n\n    with open(TRAIN_TRG_FN, 'r') as f:\n        train_trg = list(f)\n\n    with open(VAL_SRC_FN, 'r') as f:\n        val_src = list(f)\n\n    with open(VAL_TRG_FN, 'r') as f:\n        val_trg = list(f)\n\n    with open(TEST_SRC_FN, 'r') as f:\n        test_src = list(f)\n\n    with open(TEST_TRG_FN, 'r') as f:\n        test_trg = list(f)\n\n    fields = [('src', SRC), ('trg', TRG)]\n    \n    print(len(train_src))\n    train = Dataset(\n                    examples=[Example.fromlist(x, fields) for x in tqdm(zip(train_src, train_trg))]\n                    , fields=fields\n                    , filter_pred=filter_examples_with_length)\n    \n    print(len(val_src))\n    val = Dataset(\n                    examples=[Example.fromlist(x, fields) for x in tqdm(zip(val_src, val_trg))]\n                    , fields=fields\n                    , filter_pred=filter_examples_with_length)\n    \n    print(len(test_src))\n    test = Dataset(\n                    examples=[Example.fromlist(x, fields) for x in tqdm(zip(test_src, test_trg))]\n                    , fields=fields\n                    , filter_pred=filter_examples_with_length)\n\n    SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n    print('[Info] Get source language vocabulary size:', len(SRC.vocab))\n\n    TRG.build_vocab(train.trg, min_freq=MIN_FREQ)\n    print('[Info] Get target language vocabulary size:', len(TRG.vocab))\n\n    if preOpt.share_vocab:\n        print('[Info] Merging two vocabulary ...')\n        \n        for w, _ in tqdm(SRC.vocab.stoi.items()):\n            # TODO: Also update the `freq`, although it is not likely to be used.\n            if w not in TRG.vocab.stoi:\n                TRG.vocab.stoi[w] = len(TRG.vocab.stoi)\n        TRG.vocab.itos = [None] * len(TRG.vocab.stoi)\n        \n        for w, i in tqdm(TRG.vocab.stoi.items()):\n            TRG.vocab.itos[i] = w\n        SRC.vocab.stoi = TRG.vocab.stoi\n        SRC.vocab.itos = TRG.vocab.itos\n        print('[Info] Get merged vocabulary size:', len(TRG.vocab))\n\n\n    data = {\n        'settings': preOpt,\n        'vocab': {'src': SRC, 'trg': TRG},\n        'train': train.examples,\n        'valid': val.examples,\n        'test': test.examples\n    }\n\n    print('[Info] Dumping the processed data to pickle file', preOpt.save_data)\n    pickle.dump(data, open(preOpt.save_data, 'wb'))\n    return data\n\n\nif not os.path.exists(\"text2gloss_data.pkl\"):\n    preprocess()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"def cal_performance(pred, gold, trg_pad_idx, smoothing=False):\n    ''' Apply label smoothing if needed '''\n\n    loss = cal_loss(pred, gold, trg_pad_idx, smoothing=smoothing)\n\n    pred = pred.max(1)[1]\n    gold = gold.contiguous().view(-1)\n    non_pad_mask = gold.ne(trg_pad_idx)\n    n_correct = pred.eq(gold).masked_select(non_pad_mask).sum().item()\n    n_word = non_pad_mask.sum().item()\n\n    return loss, n_correct, n_word\n\n\ndef cal_loss(pred, gold, trg_pad_idx, smoothing=False):\n    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n\n    gold = gold.contiguous().view(-1)\n\n    if smoothing:\n        eps = 0.1\n        n_class = pred.size(1)\n\n        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n        log_prb = F.log_softmax(pred, dim=1)\n\n        non_pad_mask = gold.ne(trg_pad_idx)\n        loss = -(one_hot * log_prb).sum(dim=1)\n        loss = loss.masked_select(non_pad_mask).sum()  # average later\n    else:\n        loss = F.cross_entropy(pred, gold, ignore_index=trg_pad_idx, reduction='sum')\n    return loss\n\n\ndef patch_src(src, pad_idx):\n    src = src.transpose(0, 1)\n    return src\n\n\ndef patch_trg(trg, pad_idx):\n    trg = trg.transpose(0, 1)\n    trg, gold = trg[:, :-1], trg[:, 1:].contiguous().view(-1)\n    return trg, gold\n\n\ndef train_epoch(model, training_data, optimizer, opt, device, smoothing):\n    ''' Epoch operation in training phase'''\n\n    model.train()\n    total_loss, n_word_total, n_word_correct = 0, 0, 0 \n\n    desc = '  - (Training)   '\n    for batch in tqdm(training_data, mininterval=2, desc=desc, leave=False):\n\n        # prepare data\n        src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n        trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n\n        # forward\n        optimizer.zero_grad()\n        pred = model(src_seq, trg_seq)\n\n        # backward and update parameters\n        loss, n_correct, n_word = cal_performance(\n            pred, gold, opt.trg_pad_idx, smoothing=smoothing) \n        loss.backward()\n        optimizer.step_and_update_lr()\n\n        # note keeping\n        n_word_total += n_word\n        n_word_correct += n_correct\n        total_loss += loss.item()\n\n    loss_per_word = total_loss/n_word_total\n    accuracy = n_word_correct/n_word_total\n    return loss_per_word, accuracy\n\n\ndef eval_epoch(model, validation_data, device, opt):\n    ''' Epoch operation in evaluation phase '''\n\n    model.eval()\n    total_loss, n_word_total, n_word_correct = 0, 0, 0\n\n    desc = '  - (Validation) '\n    with torch.no_grad():\n        for batch in tqdm(validation_data, mininterval=2, desc=desc, leave=False):\n\n            # prepare data\n            src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n            trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n\n            # forward\n            pred = model(src_seq, trg_seq)\n            loss, n_correct, n_word = cal_performance(\n                pred, gold, opt.trg_pad_idx, smoothing=False)\n\n            # note keeping\n            n_word_total += n_word\n            n_word_correct += n_correct\n            total_loss += loss.item()\n\n    loss_per_word = total_loss/n_word_total\n    accuracy = n_word_correct/n_word_total\n    return loss_per_word, accuracy\n\n\ndef train(model, training_data, validation_data, optimizer, device, opt):\n    ''' Start training '''\n\n    log_train_file, log_valid_file = None, None\n\n    if opt.log:\n        log_train_file = opt.log + '.train.log'\n        log_valid_file = opt.log + '.valid.log'\n\n        print('[Info] Training performance will be written to file: {} and {}'.format(\n            log_train_file, log_valid_file))\n\n        with open(log_train_file, 'w') as log_tf, open(log_valid_file, 'w') as log_vf:\n            log_tf.write('epoch,loss,ppl,accuracy\\n')\n            log_vf.write('epoch,loss,ppl,accuracy\\n')\n\n    def print_performances(header, loss, accu, start_time):\n        print('  - {header:12} ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, '\\\n              'elapse: {elapse:3.3f} min'.format(\n                  header=f\"({header})\", ppl=math.exp(min(loss, 100)),\n                  accu=100*accu, elapse=(time.time()-start_time)/60))\n\n    #valid_accus = []\n    valid_losses = []\n    for epoch_i in range(opt.epoch):\n        print('[ Epoch', epoch_i, ']')\n\n        start = time.time()\n        train_loss, train_accu = train_epoch(\n            model, training_data, optimizer, opt, device, smoothing=opt.label_smoothing)\n        print_performances('Training', train_loss, train_accu, start)\n\n        start = time.time()\n        valid_loss, valid_accu = eval_epoch(model, validation_data, device, opt)\n        print_performances('Validation', valid_loss, valid_accu, start)\n\n        valid_losses += [valid_loss]\n\n        checkpoint = {'epoch': epoch_i, 'settings': opt, 'model': model.state_dict()}\n\n        if opt.save_model:\n            if opt.save_mode == 'all':\n                model_name = opt.save_model + '_accu_{accu:3.3f}.chkpt'.format(accu=100*valid_accu)\n                torch.save(checkpoint, model_name)\n            elif opt.save_mode == 'best':\n                model_name = opt.save_model + '.chkpt'\n                if valid_loss <= min(valid_losses):\n                    torch.save(checkpoint, model_name)\n                    print('    - [Info] The checkpoint file has been updated.')\n\n        if log_train_file and log_valid_file:\n            with open(log_train_file, 'a') as log_tf, open(log_valid_file, 'a') as log_vf:\n                log_tf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n                    epoch=epoch_i, loss=train_loss,\n                    ppl=math.exp(min(train_loss, 100)), accu=100*train_accu))\n                log_vf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n                    epoch=epoch_i, loss=valid_loss,\n                    ppl=math.exp(min(valid_loss, 100)), accu=100*valid_accu))\n\ndef start_training():\n    \n    class Settings:\n        data_pkl = \"text2gloss_data.pkl\"\n\n        # bpe encoded data\n        train_path = None\n        val_path = None\n\n        epoch = 200\n        b = batch_size = 256\n\n        d_model = 512\n        d_inner_hid = 1024\n        d_k = 64\n        d_v = 64\n\n        n_head = 8\n        n_layers = 6\n        warmup = n_warmup_steps = 128000\n\n        dropout = 0.1 \n        embs_share_weight = True\n        proj_share_weight = True\n\n        log = \"eng2gloss\"\n        save_model = \"trained\"\n        save_mode = \"best\"\n\n        no_cuda = True\n        label_smoothing = True\n\n    opt = Settings()\n    opt.cuda = not opt.no_cuda\n    opt.d_word_vec = opt.d_model\n\n    if not opt.log and not opt.save_model:\n        print('No experiment result will be saved.')\n        raise\n\n    if opt.batch_size < 2048 and opt.n_warmup_steps <= 4000:\n        print('[Warning] The warmup steps may be not enough.\\n'\\\n              '(sz_b, warmup) = (2048, 4000) is the official setting.\\n'\\\n              'Using smaller batch w/o longer warmup may cause '\\\n              'the warmup stage ends with only little data trained.')\n\n    device = torch.device('cuda' if opt.cuda else 'cpu')\n\n    #========= Loading Dataset =========#\n\n    if all((opt.train_path, opt.val_path)):\n        training_data, validation_data = prepare_dataloaders_from_bpe_files(opt, device)\n    elif opt.data_pkl:\n        training_data, validation_data = prepare_dataloaders(opt, device)\n    else:\n        raise\n\n    transformer = Transformer(\n        opt.src_vocab_size,\n        opt.trg_vocab_size,\n        src_pad_idx=opt.src_pad_idx,\n        trg_pad_idx=opt.trg_pad_idx,\n        trg_emb_prj_weight_sharing=opt.proj_share_weight,\n        emb_src_trg_weight_sharing=opt.embs_share_weight,\n        d_k=opt.d_k,\n        d_v=opt.d_v,\n        d_model=opt.d_model,\n        d_word_vec=opt.d_word_vec,\n        d_inner=opt.d_inner_hid,\n        n_layers=opt.n_layers,\n        n_head=opt.n_head,\n        dropout=opt.dropout).to(device)\n\n    optimizer = ScheduledOptim(\n        optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09),\n        2.0, opt.d_model, opt.n_warmup_steps)\n\n    train(transformer, training_data, validation_data, optimizer, device, opt)\n\n\ndef prepare_dataloaders_from_bpe_files(opt, device):\n    batch_size = opt.batch_size\n    MIN_FREQ = 2\n    if not opt.embs_share_weight:\n        raise\n\n    data = pickle.load(open(opt.data_pkl, 'rb'))\n    MAX_LEN = data['settings'].max_len\n    field = data['vocab']\n    fields = (field, field)\n\n    def filter_examples_with_length(x):\n        return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n\n    train = TranslationDataset(\n        fields=fields,\n        path=opt.train_path, \n        exts=('.src', '.trg'),\n        filter_pred=filter_examples_with_length)\n    val = TranslationDataset(\n        fields=fields,\n        path=opt.val_path, \n        exts=('.src', '.trg'),\n        filter_pred=filter_examples_with_length)\n\n    opt.max_token_seq_len = MAX_LEN + 2\n    opt.src_pad_idx = opt.trg_pad_idx = field.vocab.stoi[Constants.PAD_WORD]\n    opt.src_vocab_size = opt.trg_vocab_size = len(field.vocab)\n\n    train_iterator = BucketIterator(train, batch_size=batch_size, device=device, train=True)\n    val_iterator = BucketIterator(val, batch_size=batch_size, device=device)\n    return train_iterator, val_iterator\n\n\ndef prepare_dataloaders(opt, device):\n    batch_size = opt.batch_size\n    data = pickle.load(open(opt.data_pkl, 'rb'))\n\n    opt.max_token_seq_len = data['settings'].max_len\n    opt.src_pad_idx = data['vocab']['src'].vocab.stoi[Constants.PAD_WORD]\n    opt.trg_pad_idx = data['vocab']['trg'].vocab.stoi[Constants.PAD_WORD]\n\n    opt.src_vocab_size = len(data['vocab']['src'].vocab)\n    opt.trg_vocab_size = len(data['vocab']['trg'].vocab)\n\n    #========= Preparing Model =========#\n    if opt.embs_share_weight:\n        assert data['vocab']['src'].vocab.stoi == data['vocab']['trg'].vocab.stoi, \\\n            'To sharing word embedding the src/trg word2idx table shall be the same.'\n\n    fields = {'src': data['vocab']['src'], 'trg':data['vocab']['trg']}\n\n    train = Dataset(examples=data['train'], fields=fields)\n    val = Dataset(examples=data['valid'], fields=fields)\n\n    train_iterator = BucketIterator(train, batch_size=batch_size, device=device, train=True)\n    val_iterator = BucketIterator(val, batch_size=batch_size, device=device)\n\n    return train_iterator, val_iterator\n\n\nstart_training()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Translate","metadata":{}},{"cell_type":"code","source":"def load_model(opt, device):\n\n    checkpoint = torch.load(opt.model, map_location=device)\n    model_opt = checkpoint['settings']\n\n    model = Transformer(\n        model_opt.src_vocab_size,\n        model_opt.trg_vocab_size,\n\n        model_opt.src_pad_idx,\n        model_opt.trg_pad_idx,\n\n        trg_emb_prj_weight_sharing=model_opt.proj_share_weight,\n        emb_src_trg_weight_sharing=model_opt.embs_share_weight,\n        d_k=model_opt.d_k,\n        d_v=model_opt.d_v,\n        d_model=model_opt.d_model,\n        d_word_vec=model_opt.d_word_vec,\n        d_inner=model_opt.d_inner_hid,\n        n_layers=model_opt.n_layers,\n        n_head=model_opt.n_head,\n        dropout=model_opt.dropout).to(device)\n\n    model.load_state_dict(checkpoint['model'])\n    print('[Info] Trained model state loaded.')\n    return model \n\n\ndef translate():\n    \n    class Settings():\n        model = \"text2gloss.model\"\n        data_pkl = \"text2gloss_data.pkl\"\n\n        input = \"ASLG-PC12/ENG-ASL_Test.en\"\n        expected = \"ASLG-PC12/ENG-ASL_Test.asl\"\n        output = \"prediction.txt\"\n\n        beam_size = 5\n        max_seq_len = 100\n        no_cuda = True\n    \n    opt = Settings()\n    opt.cuda = not opt.no_cuda\n\n    data = pickle.load(open(opt.data_pkl, 'rb'))\n    SRC, TRG = data['vocab']['src'], data['vocab']['trg']\n    opt.src_pad_idx = SRC.vocab.stoi[Constants.PAD_WORD]\n    opt.trg_pad_idx = TRG.vocab.stoi[Constants.PAD_WORD]\n    opt.trg_bos_idx = TRG.vocab.stoi[Constants.BOS_WORD]\n    opt.trg_eos_idx = TRG.vocab.stoi[Constants.EOS_WORD]\n\n    with open(opt.input, 'r') as f:\n        translate_src = f.readlines()\n    \n    with open(opt.expected, 'r') as f:\n        expected_src = f.readlines() \n    \n    fields = [('src', SRC), ('trg', TRG)]\n    \n    print(len(translate_src))\n    print(len(expected_src))\n#     data_loader = Dataset(examples=[Example.fromlist(x, fields) \n#                     for x in tqdm(zip(translate_src, expected_src))], fields={'src': SRC, 'trg': TRG})\n\n    device = torch.device('cuda' if opt.cuda else 'cpu')\n    translator = Translator(\n        model=load_model(opt, device),\n        beam_size=opt.beam_size,\n        max_seq_len=opt.max_seq_len,\n        src_pad_idx=opt.src_pad_idx,\n        trg_pad_idx=opt.trg_pad_idx,\n        trg_bos_idx=opt.trg_bos_idx,\n        trg_eos_idx=opt.trg_eos_idx).to(device)\n\n    unk_idx = SRC.vocab.stoi[SRC.unk_token]\n \n    with open(opt.output, 'w') as f:\n        for tra, exp in tqdm(zip(translate_src, expected_src), mininterval=2, desc='  - (Test)', leave=False):\n            tra = tra.split()\n            exp = exp.split()\n            src_seq = [SRC.vocab.stoi.get(word, unk_idx) for word in (tra + [\".\"])]\n            pred_seq = translator.translate_sentence(torch.LongTensor([src_seq]).to(device))\n            pred_line = ' '.join(TRG.vocab.itos[idx] for idx in pred_seq)\n            pred_line = pred_line.replace(Constants.BOS_WORD, '').replace(Constants.EOS_WORD, '')\n\n#             print('\\n')\n#             print('SRC', ' '.join(tra))\n#             print('TRG', ' '.join(exp))\n#             print('PRED', pred_line)\n\n            f.write(pred_line.strip() + '\\n')\n\n    print('[Info] Finished.')\n\n\n# cont = \"\"\"She drives a car.\n# Parliament is now in session.\n# We like to eat pizza.\n# We have received the documents.\n# We have opened the court.\n# The doctor appointment is soon.\n# I forwarded the texts to you.\n# \"\"\"\n\n# with open(\"translate_src.txt\", \"w\") as f:\n#     f.write(cont)\n\n\ntranslate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Score","metadata":{}},{"cell_type":"code","source":"def get_score():\n    \n    class Settings:\n        data_pkl = \"text2gloss_data.pkl\"\n        trg_data = \"ASLG-PC12/ENG-ASL_Test.asl\"\n        pred_data = \"prediction.txt\"\n  \n    opt = Settings()\n    data = pickle.load(open(opt.data_pkl, 'rb'))\n    SRC, TRG = data['vocab']['src'], data['vocab']['trg']\n\n    fields = [('src', SRC)]\n\n    with open(opt.trg_data, 'r') as f:\n        trg_loader = Dataset(examples=[Example.fromlist([x], fields) for x in f], fields={'src': SRC})\n    trg_txt = [x.src for x in trg_loader]\n\n    with open(opt.pred_data, 'r') as f:\n        pred_loader = Dataset(examples=[Example.fromlist([x], fields) for x in f], fields={'src': SRC})\n    pred_txt = [[x.src] for x in pred_loader]\n    \n    if len(pred_txt) != len(trg_txt):\n        print(\"Slicing\")\n        trg_txt = trg_txt[:len(pred_txt)]\n\n    print(\"Scoring...\")\n    score = bleu_score(trg_txt, pred_txt)\n    print('Bleu 4 score is {}'.format(str(score)))\n\n\nget_score()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# #","metadata":{}},{"cell_type":"code","source":"data = pickle.load(open(\"text2gloss_data.pkl\", 'rb'))\nSRC, TRG = data['vocab']['src'], data['vocab']['trg']\n\nclass Settings():\n    model = \"text2gloss.model\"\n    data_pkl = \"text2gloss_data.pkl\"\n\n    beam_size = 5\n    max_seq_len = 100\n    no_cuda = True\n\nopt = Settings()\nopt.cuda = not opt.no_cuda\nopt.src_pad_idx = SRC.vocab.stoi[Constants.PAD_WORD]\nopt.trg_pad_idx = TRG.vocab.stoi[Constants.PAD_WORD]\nopt.trg_bos_idx = TRG.vocab.stoi[Constants.BOS_WORD]\nopt.trg_eos_idx = TRG.vocab.stoi[Constants.EOS_WORD]\n\ndevice = torch.device('cuda' if opt.cuda else 'cpu')\n\ntranslator = Translator(\n    model=load_model(opt, device),\n    beam_size=opt.beam_size,\n    max_seq_len=opt.max_seq_len,\n    src_pad_idx=opt.src_pad_idx,\n    trg_pad_idx=opt.trg_pad_idx,\n    trg_bos_idx=opt.trg_bos_idx,\n    trg_eos_idx=opt.trg_eos_idx).to(device)\n\nunk_idx = SRC.vocab.stoi[SRC.unk_token]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spoken = \"membership of parliament see minutes\".lower().strip().split()\nspoken.append(\".\")\nprint(spoken)\n\nsrc_seq = [SRC.vocab.stoi.get(word, unk_idx) for word in spoken]\npred_seq = translator.translate_sentence(torch.LongTensor([src_seq]).to(device))\npred_line = ' '.join(TRG.vocab.itos[idx] for idx in pred_seq)\npred_line = pred_line.replace(Constants.BOS_WORD, '').replace(Constants.EOS_WORD, '')\npred_line.strip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}