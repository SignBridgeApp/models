{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Eng to ASL (SignWriting) and Visuailazation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-7KAtux3Zhz",
        "outputId": "bc36e994-667d-4871-8562-ed0bb0b86e8b"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sign-language-processing/signbank-plus\n",
        "# !cd signbank-plus/signbank_plus/nmt/ && git clone https://github.com/J22Melody/signwriting-translation\n",
        "%pip install -r signbank-plus/requirements.txt\n",
        "%pip install subword-nmt OpenNMT-py==1.2.0 install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import gzip\n",
        "import itertools\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "from tqdm import tqdm\n",
        "import importlib\n",
        "module_name = 'signbank-plus.signbank_plus.load_data'\n",
        "load_data_fol = importlib.import_module(module_name)\n",
        "load_data = load_data_fol.load_data\n",
        "load_file = load_data_fol.load_file\n",
        "from signwriting.tokenizer import SignWritingTokenizer\n",
        "\n",
        "from shutil import copy2\n",
        "import subprocess\n",
        "from os.path import exists, join, isfile\n",
        "from os import listdir\n",
        "import re\n",
        "\n",
        "\n",
        "def run_command(command):\n",
        "    print(\"\\n\" + command + \"\\n\")\n",
        "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')\n",
        "    process.wait()\n",
        "    if process.returncode != 0:\n",
        "        raise subprocess.CalledProcessError(process.returncode, command)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Edva_pN14q5V",
        "outputId": "7a4eba23-ed03-4024-bf64-a16cd6642d2d"
      },
      "outputs": [],
      "source": [
        "ALL_FLAGS = set()\n",
        "\n",
        "def get_source_target(data, field=\"annotated_texts\"):\n",
        "    random.Random(42).shuffle(data)  # Shuffle data consistently\n",
        "    for instance in data:\n",
        "        if field in instance:\n",
        "            for text in instance[field]:\n",
        "                if len(text.strip()) > 0 and len(instance[\"sign_writing\"].strip()) > 0:\n",
        "                    yield {\n",
        "                        \"puddle_id\": instance[\"puddle_id\"] if \"puddle_id\" in instance else None,\n",
        "                        \"example_id\": instance[\"example_id\"] if \"example_id\" in instance else None,\n",
        "                        \"flags\": [instance[\"spoken_language\"], instance[\"sign_language\"]],\n",
        "                        \"source\": instance[\"sign_writing\"].strip(),\n",
        "                        \"target\": text.strip(),\n",
        "                    }\n",
        "\n",
        "\n",
        "def get_source_target_no_test(data, field=\"annotated_texts\"):\n",
        "    test_instances = load_data(\"benchmark\")\n",
        "    test_instances = {(instance['puddle_id'], instance['example_id']) for instance in test_instances}\n",
        "    for instance in get_source_target(data, field):\n",
        "        if (instance['puddle_id'], instance['example_id']) not in test_instances:\n",
        "            yield instance\n",
        "\n",
        "\n",
        "# Model 1: Original data\n",
        "def get_original_data():\n",
        "    data = load_data(\"raw\")\n",
        "    yield from get_source_target_no_test(data, field=\"texts\")\n",
        "\n",
        "\n",
        "# Model 2: Cleaned data\n",
        "def get_cleaned_data():\n",
        "    data = load_data(\"raw\", \"gpt-3.5-cleaned\", \"manually-cleaned\", \"bible\")\n",
        "    yield from get_source_target_no_test(data, field=\"annotated_texts\")\n",
        "\n",
        "\n",
        "# Model 3: Expanded data\n",
        "def get_expanded_data():\n",
        "    data = load_data(\"raw\", \"gpt-3.5-cleaned\", \"gpt-3.5-expanded\", \"manually-cleaned\", \"bible\")\n",
        "    yield from get_source_target_no_test(data, field=\"annotated_texts\")\n",
        "\n",
        "\n",
        "def get_expanded_data_en():\n",
        "    data = load_data(\"gpt-3.5-expanded.en\")\n",
        "    yield from get_source_target_no_test(data, field=\"annotated_texts\")\n",
        "\n",
        "\n",
        "def test_set():\n",
        "    data = load_file(\"benchmark\", array_fields=[\"gold_texts\"])\n",
        "    yield from get_source_target(data, field=\"gold_texts\")\n",
        "\n",
        "\n",
        "def save_parallel_csv(path: Path, data: iter, split=\"train\", extra_flags=[]):\n",
        "    for flag in extra_flags:\n",
        "        ALL_FLAGS.add(flag)\n",
        "\n",
        "    f_source = open(f\"{path}/{split}.source\", \"w\", encoding=\"utf-8\")\n",
        "    f_source_tokenized = open(f\"{path}/{split}.source.tokenized\", \"w\", encoding=\"utf-8\")\n",
        "    f_target = open(f\"{path}/{split}.target\", \"w\", encoding=\"utf-8\")\n",
        "    f_csv = open(f\"{path}/{split}.csv\", \"w\", encoding=\"utf-8\")\n",
        "\n",
        "    f_spoken_gzip = gzip.open(path.joinpath(f'{split}.spoken.gz'), 'wt', encoding='utf-8')\n",
        "    f_signed_gzip = gzip.open(path.joinpath(f'{split}.signed.gz'), 'wt', encoding='utf-8')\n",
        "\n",
        "    tokenizer = SignWritingTokenizer()\n",
        "\n",
        "    writer = csv.DictWriter(f_csv, fieldnames=[\"source\", \"target\"])\n",
        "    writer.writeheader()\n",
        "    for instance in tqdm(data):\n",
        "        if 0 < len(instance[\"target\"]) < 512 and 0 < len(instance[\"source\"]) < 1024:\n",
        "            flag_tokens = [f\"${flag}\" for flag in instance[\"flags\"]]\n",
        "            for flag in flag_tokens:\n",
        "                ALL_FLAGS.add(flag)\n",
        "            flags = \" \".join(flag_tokens)\n",
        "\n",
        "            source = flags + \" \" + instance[\"source\"]\n",
        "            writer.writerow({\n",
        "                \"source\": source,\n",
        "                \"target\": instance[\"target\"],\n",
        "            })\n",
        "            f_source.write(source + \"\\n\")\n",
        "            f_target.write(instance[\"target\"] + \"\\n\")\n",
        "\n",
        "            tokens_source = list(tokenizer.text_to_tokens(instance[\"source\"]))\n",
        "            tokenized_source = \" \".join(tokens_source)\n",
        "            f_source_tokenized.write(flags + \" \" + tokenized_source + \"\\n\")\n",
        "\n",
        "            gzip_flags = \" \".join(extra_flags) + \" \" + flags\n",
        "            # We detokenize the SignWriting, which removes \"A\" prefixes, and box placement\n",
        "            detokenized_source = tokenizer.tokens_to_text(tokens_source)\n",
        "            f_spoken_gzip.write(gzip_flags + \" \" + instance[\"target\"] + \"\\n\")\n",
        "            f_signed_gzip.write(gzip_flags + \" \" + detokenized_source + \"\\n\")\n",
        "\n",
        "    f_source.close()\n",
        "    f_source_tokenized.close()\n",
        "    f_target.close()\n",
        "    f_csv.close()\n",
        "\n",
        "\n",
        "def save_splits(path: Path, data: iter, extra_flags: list = [], dev_num=3000):\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    if dev_num > 0:\n",
        "        save_parallel_csv(path, itertools.islice(data, dev_num), split=\"dev\", extra_flags=extra_flags)\n",
        "    save_parallel_csv(path, data, split=\"train\", extra_flags=extra_flags)\n",
        "\n",
        "\n",
        "def save_test(path: Path, data: iter):\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    save_parallel_csv(path, data, split=\"all\")\n",
        "\n",
        "    # Read source file and target file\n",
        "    with open(f\"{path}/all.source\", 'r', encoding='utf-8') as f:\n",
        "        source_lines = [l.strip() for l in f.readlines()]\n",
        "    with open(f\"{path}/all.source.tokenized\", 'r', encoding='utf-8') as f:\n",
        "        source_lines_tokenized = [l.strip() for l in f.readlines()]\n",
        "    with open(f\"{path}/all.target\", 'r', encoding='utf-8') as f:\n",
        "        target_lines = [l.strip() for l in f.readlines()]\n",
        "\n",
        "    source_map = {source_tokenized: source for source_tokenized, source in zip(source_lines_tokenized, source_lines)}\n",
        "\n",
        "    source_target_map = defaultdict(list)\n",
        "    for source, target in zip(source_lines_tokenized, target_lines):\n",
        "        source_target_map[source].append(target)\n",
        "\n",
        "    max_references = max(len(references) for references in source_target_map.values())\n",
        "    print(f\"Max test references: {max_references}\")\n",
        "\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(f\"{path}/test.source.unique\", 'w') as f1:\n",
        "        with open(f\"{path}/test.source.unique.tokenized\", 'w') as f2:\n",
        "            for source, references in source_target_map.items():\n",
        "                f1.write(source_map[source])\n",
        "                f1.write(\"\\n\")\n",
        "                f2.write(source)\n",
        "                f2.write(\"\\n\")\n",
        "\n",
        "    for i in range(max_references):\n",
        "        with open(f\"{path}/test.target.{i}\", 'w', encoding='utf-8') as f:\n",
        "            for source, references in source_target_map.items():\n",
        "                if len(references) > i:\n",
        "                    f.write(references[i])\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "\n",
        "if True:\n",
        "    parallel_path = Path(\"signbank-plus/data/parallel\")\n",
        "\n",
        "    save_test(parallel_path / \"test\", test_set())\n",
        "\n",
        "    save_splits(parallel_path / \"original\", get_original_data())\n",
        "    save_splits(parallel_path / \"cleaned\", get_cleaned_data())\n",
        "    save_splits(parallel_path / \"expanded\", itertools.chain.from_iterable([\n",
        "        get_expanded_data(),\n",
        "        get_expanded_data_en()\n",
        "    ]))\n",
        "\n",
        "    save_splits(parallel_path / \"more\", itertools.chain.from_iterable([\n",
        "        get_source_target(load_data(\"sign2mint\"), field=\"texts\"),\n",
        "        get_source_target(load_data(\"signsuisse\"), field=\"texts\"),\n",
        "        get_source_target(load_data(\"fingerspelling\"), field=\"texts\"),\n",
        "    ]), dev_num=0)\n",
        "\n",
        "    print(\"\\n\" + \",\".join(ALL_FLAGS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLjoZXUR5Vb9",
        "outputId": "8c7548c5-677b-4c99-f3c5-fdaa20b49eda"
      },
      "outputs": [],
      "source": [
        "def find_latest_checkpoint(checkpoint_dir):\n",
        "    checkpoint_files = [f for f in listdir(checkpoint_dir) if re.match(r'checkpoint_step_\\d+\\.pt', f)]\n",
        "    if not checkpoint_files:\n",
        "        return None\n",
        "    checkpoint_files.sort()\n",
        "    latest_checkpoint = join(checkpoint_dir, checkpoint_files[-2 if len(checkpoint_files) > 1 else -1])\n",
        "    return latest_checkpoint\n",
        "\n",
        "\n",
        "def train(_1, _2, _3 = None):\n",
        "    _2t = Path(_2)\n",
        "    _2t.parent.mkdir(parents=True, exist_ok=True)\n",
        "    _2t.mkdir(exist_ok=True)\n",
        "    (_2t / \"data\").mkdir(exist_ok=True)\n",
        "    (_2t / \"model\").mkdir(exist_ok=True)\n",
        "    \n",
        "    if _3 is not None:\n",
        "        copy2(f\"{_3}/bpe.codes.target\", f\"{_2}/bpe.codes.target\")\n",
        "        \n",
        "    \n",
        "    # Target BPE\n",
        "    if not exists(f\"{_2}/bpe.codes.target\"):\n",
        "        command = f\"subword-nmt learn-bpe -s 3000 < {_1}/train.target > {_2}/bpe.codes.target\"\n",
        "        run_command(command)\n",
        "    \n",
        "    if not exists(f\"{_2}/data/train.target\"):\n",
        "        command = f\"subword-nmt apply-bpe -c {_2}/bpe.codes.target < {_1}/train.target > {_2}/data/train.target\"\n",
        "        run_command(command)\n",
        "        \n",
        "    if not exists(f\"{_2}/data/dev.target\"):\n",
        "        command = f\"subword-nmt apply-bpe -c {_2}/bpe.codes.target < {_1}/dev.target > {_2}/data/dev.target\"\n",
        "        run_command(command)\n",
        "        \n",
        "    # Copy source\n",
        "    if not exists(f\"{_2}/data/train.source\"):\n",
        "        copy2(f\"{_1}/train.source.tokenized\", f\"{_2}/data/train.source\")\n",
        "    \n",
        "    if not exists(f\"{_2}/data/dev.source\"):\n",
        "        copy2(f\"{_1}/dev.source.tokenized\", f\"{_2}/data/dev.source\")\n",
        "\n",
        "\n",
        "    if not exists(f\"{_2}/processed.vocab.pt\"):\n",
        "        command = f\"onmt_preprocess --save_data {_2}/processed --shard_size 2000000 \\\n",
        "        --train_src {_2}/data/train.source --train_tgt {_2}/data/train.target \\\n",
        "        --valid_src {_2}/data/dev.source --valid_tgt {_2}/data/dev.target \\\n",
        "        --src_seq_length 512 \\\n",
        "        --tgt_seq_length 512 \\\n",
        "        --log_file_level DEBUG\"\n",
        "        run_command(command)\n",
        "\n",
        "    train_from_param = \"\"\n",
        "    latest_checkpoint = find_latest_checkpoint(f\"{_2}/model/\")\n",
        "    if latest_checkpoint:\n",
        "        train_from_param = f\"--train_from {latest_checkpoint}\"\n",
        "\n",
        "    command = f\"onmt_train --data {_2}/processed --save_model {_2}/model/checkpoint \\\n",
        "    {train_from_param} --layers 2 --rnn_size 512 --word_vec_size 512 --heads 8 \\\n",
        "    --encoder_type transformer --decoder_type transformer --position_encoding --transformer_ff 2048 --dropout 0.1 \\\n",
        "    --early_stopping 10 --early_stopping_criteria accuracy ppl --batch_size 2048 --accum_count 3 --batch_type tokens \\\n",
        "    --max_generator_batches 2 --normalization tokens --optim adam --adam_beta2 0.998 --decay_method noam \\\n",
        "    --warmup_steps 3000 --learning_rate 0.5 --max_grad_norm 0 --param_init 0 --param_init_glorot --label_smoothing 0.1 \\\n",
        "    --valid_steps 500 --save_checkpoint_steps 500 --world_size 1 --gpu_ranks 0\"\n",
        "    run_command(command)\n",
        "    \n",
        "\n",
        "train(\"signbank-plus/data/parallel/original\", \"opennmt/original\")\n",
        "train(\"signbank-plus/data/parallel/cleaned\", \"opennmt/cleaned\")\n",
        "train(\"signbank-plus/data/parallel/expanded\", \"opennmt/expanded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evauluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTcnBDRdBd1B"
      },
      "outputs": [],
      "source": [
        "def find_best_model(_2):\n",
        "    model_files = listdir(join(_2, \"model\"))\n",
        "    model_indices = [int(file.split('_')[2]) for file in model_files]\n",
        "    sorted_models = sorted(zip(model_indices, model_files), key=lambda x: x[0])\n",
        "    \n",
        "    best_model = sorted_models[-11][1] if len(sorted_models) >= 11 else None\n",
        "    return best_model\n",
        "\n",
        "\n",
        "def evall(_1, _2):\n",
        "    best_model = find_best_model(_2)\n",
        "\n",
        "    if best_model is None:\n",
        "        print(\"Error: Unable to find the best model.\")\n",
        "        return\n",
        "\n",
        "    # Translating\n",
        "    translations_bpe_path = join(_2, \"test.translations.bpe\")\n",
        "    if not isfile(translations_bpe_path):\n",
        "        translate_command = f\"onmt_translate --model {join(_2, 'model', best_model)} \\\n",
        "        --src {join(_1, 'test.source.unique.tokenized')} \\\n",
        "        --output {translations_bpe_path} \\\n",
        "        --gpu 0 --replace_unk --beam_size 5\"\n",
        "        run_command(translate_command)\n",
        "\n",
        "    # Removing BPE\n",
        "    translations_path = join(_2, \"test.translations\")\n",
        "    with open(translations_bpe_path, 'r') as f_in, open(translations_path, 'w') as f_out:\n",
        "        for line in f_in:\n",
        "            f_out.write(line.replace(\"@@ \", \"\").replace(\"@@\", \"\"))\n",
        "\n",
        "    # Computing BLEU and CHR F\n",
        "    sacrebleu_output_path = join(_2, \"sacrebleu.txt\")\n",
        "    sacrebleu_command = f\"sacrebleu $(find {_1} -type f -name 'test.target*') -i {translations_path} -m bleu chrf --width 2 > {sacrebleu_output_path}\"\n",
        "    run_command(sacrebleu_command)\n",
        "   \n",
        "    \n",
        "evall(\"signbank-plus/data/parallel/test\", \"opennmt/original\")\n",
        "evall(\"signbank-plus/data/parallel/test\", \"opennmt/cleaned\")\n",
        "evall(\"signbank-plus/data/parallel/test\", \"opennmt/expanded\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
